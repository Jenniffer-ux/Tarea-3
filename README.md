
# ğŸ§  Tarea: OptimizaciÃ³n de Precios con Redes Neuronales

Este proyecto implementa una red neuronal desde cero utilizando Ãºnicamente NumPy, con el objetivo de predecir la demanda de productos en funciÃ³n de sus caracterÃ­sticas (precio, categorÃ­a, stock, etc.).

## ğŸ“ Estructura del Repositorio

```
tarea-redes-neuronales-final/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ synthetic_data.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_implementacion_red.ipynb
â”‚   â”œâ”€â”€ 02_experimentacion.ipynb
â”‚   â””â”€â”€ 03_analisis_resultados.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ neural_network.py
â”‚   â””â”€â”€ data_preprocessing.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ demanda_prediccion.png
â”‚   â””â”€â”€ architecture_analysis.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ CÃ³mo ejecutar

1. Clona el repositorio o sÃºbelo a Colab
2. Ejecuta:

```bash
pip install -r requirements.txt
```

3. Corre los notebooks en orden:
   - `01_implementacion_red.ipynb`
   - `02_experimentacion.ipynb`
   - `03_analisis_resultados.ipynb`

## ğŸ“Š Resultados

ComparaciÃ³n de arquitecturas y activaciones para predicciÃ³n de demanda (MSE â†“):

![ComparaciÃ³n de Modelos](results/architecture_analysis.png)

- La **RegresiÃ³n Lineal** tuvo el menor error.
- La red neuronal con `tanh` logrÃ³ resultados competitivos.
- Las activaciones `sigmoid` y `relu` necesitaron ajustes para mejorar.

## ğŸ§  Lecciones Aprendidas

- El entrenamiento desde cero puede causar inestabilidad numÃ©rica (`NaN`) si no se ajusta bien el `learning_rate`.
- Activaciones como `tanh` son mÃ¡s estables en redes pequeÃ±as.
- A pesar de que modelos simples como regresiÃ³n lineal pueden funcionar mejor en problemas lineales, las redes neuronales son Ãºtiles en escenarios mÃ¡s complejos.

## ğŸš€ Mejoras Futuras

- Migrar el modelo a TensorFlow o PyTorch
- AÃ±adir optimizadores como Adam
- Incorporar regularizaciÃ³n o dropout
- Desplegar como API para predicciones dinÃ¡micas
